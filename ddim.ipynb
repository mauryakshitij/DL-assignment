{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc9c49d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-06T19:45:37.563716Z",
     "iopub.status.busy": "2024-09-06T19:45:37.562716Z",
     "iopub.status.idle": "2024-09-06T19:45:37.568443Z",
     "shell.execute_reply": "2024-09-06T19:45:37.567693Z"
    },
    "papermill": {
     "duration": 0.015244,
     "end_time": "2024-09-06T19:45:37.570339",
     "exception": false,
     "start_time": "2024-09-06T19:45:37.555095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f37701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:45:37.582977Z",
     "iopub.status.busy": "2024-09-06T19:45:37.582626Z",
     "iopub.status.idle": "2024-09-06T19:46:05.591538Z",
     "shell.execute_reply": "2024-09-06T19:46:05.590335Z"
    },
    "papermill": {
     "duration": 28.017979,
     "end_time": "2024-09-06T19:46:05.594154",
     "exception": false,
     "start_time": "2024-09-06T19:45:37.576175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.4.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchvision) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f35ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:05.608109Z",
     "iopub.status.busy": "2024-09-06T19:46:05.607722Z",
     "iopub.status.idle": "2024-09-06T19:46:24.505717Z",
     "shell.execute_reply": "2024-09-06T19:46:24.504881Z"
    },
    "papermill": {
     "duration": 18.907861,
     "end_time": "2024-09-06T19:46:24.508253",
     "exception": false,
     "start_time": "2024-09-06T19:46:05.600392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathlib2\r\n",
      "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from pathlib2) (1.16.0)\r\n",
      "Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\r\n",
      "Installing collected packages: pathlib2\r\n",
      "Successfully installed pathlib2-2.3.7.post1\r\n"
     ]
    }
   ],
   "source": [
    "#importing all the necessary libraries\n",
    "!pip install pathlib2\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Optional, Union\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from pathlib2 import Path\n",
    "\n",
    "from typing import Tuple, Union, Dict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a91599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.522924Z",
     "iopub.status.busy": "2024-09-06T19:46:24.522456Z",
     "iopub.status.idle": "2024-09-06T19:46:24.578740Z",
     "shell.execute_reply": "2024-09-06T19:46:24.577916Z"
    },
    "papermill": {
     "duration": 0.065543,
     "end_time": "2024-09-06T19:46:24.580750",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.515207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f906da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.594797Z",
     "iopub.status.busy": "2024-09-06T19:46:24.594456Z",
     "iopub.status.idle": "2024-09-06T19:46:24.602221Z",
     "shell.execute_reply": "2024-09-06T19:46:24.601275Z"
    },
    "papermill": {
     "duration": 0.017197,
     "end_time": "2024-09-06T19:46:24.604299",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.587102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the config\n",
    "\n",
    "config = {\n",
    "    \"Model\": {\n",
    "        \"in_channels\": 3,\n",
    "        \"out_channels\": 3,\n",
    "        \"model_channels\": 128,\n",
    "        \"attention_resolutions\": [2],\n",
    "        \"num_res_blocks\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"channel_mult\": [1, 2, 2, 2],\n",
    "        \"conv_resample\": True,\n",
    "        \"num_heads\": 4\n",
    "    },\n",
    "    \"Dataset\": {\n",
    "        \"dataset\": \"cifar\",\n",
    "        \"train\": True,\n",
    "        \"data_path\": \"./data\",\n",
    "        \"download\": True,\n",
    "        \"image_size\": [32, 32],\n",
    "        \"mode\": \"RGB\",\n",
    "        \"suffix\": [\"png\", \"jpg\"],\n",
    "        \"batch_size\": 64,\n",
    "        \"shuffle\": True,\n",
    "        \"drop_last\": True,\n",
    "        \"pin_memory\": True,\n",
    "        \"num_workers\": 4\n",
    "    },\n",
    "    \"Trainer\": {\n",
    "        \"T\": 1000,\n",
    "        \"beta\": [0.0001, 0.02]\n",
    "    },\n",
    "    \"Callback\": {\n",
    "        \"filepath\": \"/kaggle/working/checkpoint/cifar10.pth\",\n",
    "        \"save_freq\": 1\n",
    "    },\n",
    "    \n",
    "    \"device\": \"cuda:0\",\n",
    "    \"epochs\": 20,\n",
    "    \"consume\": False,\n",
    "    \"consume_path\": \"/kaggle/working/checkpoint/cifar10.pth\",\n",
    "    \n",
    "    \"lr\": 0.0002\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64088b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.618783Z",
     "iopub.status.busy": "2024-09-06T19:46:24.618400Z",
     "iopub.status.idle": "2024-09-06T19:46:24.657678Z",
     "shell.execute_reply": "2024-09-06T19:46:24.656750Z"
    },
    "papermill": {
     "duration": 0.048907,
     "end_time": "2024-09-06T19:46:24.659609",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.610702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#UNet\n",
    "\n",
    "# use sinusoidal position embedding to encode time step (https://arxiv.org/abs/1706.03762)\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# define TimestepEmbedSequential to support `time_emb` as extra input\n",
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Any module where forward() takes timestep embeddings as a second argument.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# use GN for norm layer\n",
    "def norm_layer(channels):\n",
    "    return nn.GroupNorm(32, channels)\n",
    "\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(TimestepBlock):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            norm_layer(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # pojection for time step embedding\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            norm_layer(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        `x` has shape `[batch_size, in_dim, height, width]`\n",
    "        `t` has shape `[batch_size, time_dim]`\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        # Add time step embeddings\n",
    "        h += self.time_emb(t)[:, :, None, None]\n",
    "        h = self.conv2(h)\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "# Attention block with shortcut\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert channels % num_heads == 0\n",
    "\n",
    "        self.norm = norm_layer(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q, k, v = qkv.reshape(B * self.num_heads, -1, H * W).chunk(3, dim=1)\n",
    "        scale = 1. / math.sqrt(math.sqrt(C // self.num_heads))\n",
    "        attn = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        h = torch.einsum(\"bts,bcs->bct\", attn, v)\n",
    "        h = h.reshape(B, -1, H, W)\n",
    "        h = self.proj(h)\n",
    "        return h + x\n",
    "\n",
    "\n",
    "# upsample\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# downsample\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(stride=2, kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "# The full UNet model with attention and timestep embedding\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels=3,\n",
    "            model_channels=128,\n",
    "            out_channels=3,\n",
    "            num_res_blocks=2,\n",
    "            attention_resolutions=(8, 16),\n",
    "            dropout=0,\n",
    "            channel_mult=(1, 2, 2, 2),\n",
    "            conv_resample=True,\n",
    "            num_heads=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # time embedding\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        # down blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1))\n",
    "        ])\n",
    "        down_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResidualBlock(ch, mult * model_channels, time_embed_dim, dropout)\n",
    "                ]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                self.down_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                down_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:  # don't use downsample for the last stage\n",
    "                self.down_blocks.append(TimestepEmbedSequential(Downsample(ch, conv_resample)))\n",
    "                down_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "\n",
    "        # middle block\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResidualBlock(ch, ch, time_embed_dim, dropout),\n",
    "            AttentionBlock(ch, num_heads=num_heads),\n",
    "            ResidualBlock(ch, ch, time_embed_dim, dropout)\n",
    "        )\n",
    "\n",
    "        # up blocks\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                layers = [\n",
    "                    ResidualBlock(\n",
    "                        ch + down_block_chans.pop(),\n",
    "                        model_channels * mult,\n",
    "                        time_embed_dim,\n",
    "                        dropout\n",
    "                    )\n",
    "                ]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                if level and i == num_res_blocks:\n",
    "                    layers.append(Upsample(ch, conv_resample))\n",
    "                    ds //= 2\n",
    "                self.up_blocks.append(TimestepEmbedSequential(*layers))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            norm_layer(ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(model_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x H x W] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        # time step embedding\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        # down stage\n",
    "        h = x\n",
    "        for module in self.down_blocks:\n",
    "            h = module(h, emb)\n",
    "            hs.append(h)\n",
    "        # middle stage\n",
    "        h = self.middle_block(h, emb)\n",
    "        # up stage\n",
    "        for module in self.up_blocks:\n",
    "            cat_in = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = module(cat_in, emb)\n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17dcdc54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.673652Z",
     "iopub.status.busy": "2024-09-06T19:46:24.673321Z",
     "iopub.status.idle": "2024-09-06T19:46:24.680316Z",
     "shell.execute_reply": "2024-09-06T19:46:24.679487Z"
    },
    "papermill": {
     "duration": 0.016241,
     "end_time": "2024-09-06T19:46:24.682234",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.665993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mnist_dataset(data_path, batch_size, **kwargs):\n",
    "    train = kwargs.get(\"train\", True)\n",
    "    download = kwargs.get(\"download\", True)\n",
    "\n",
    "    dataset = MNIST(root=data_path, train=train, download=download, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ]))\n",
    "\n",
    "    loader_params = dict(\n",
    "        shuffle=kwargs.get(\"shuffle\", True),\n",
    "        drop_last=kwargs.get(\"drop_last\", True),\n",
    "        pin_memory=kwargs.get(\"pin_memory\", True),\n",
    "        num_workers=kwargs.get(\"num_workers\", 4),\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, **loader_params)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a412a97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.695877Z",
     "iopub.status.busy": "2024-09-06T19:46:24.695519Z",
     "iopub.status.idle": "2024-09-06T19:46:24.702788Z",
     "shell.execute_reply": "2024-09-06T19:46:24.701937Z"
    },
    "papermill": {
     "duration": 0.016412,
     "end_time": "2024-09-06T19:46:24.704723",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.688311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def create_cifar10_dataset(data_path, batch_size, **kwargs):\n",
    "    train = kwargs.get(\"train\", True)\n",
    "    download = kwargs.get(\"download\", True)\n",
    "\n",
    "    dataset = CIFAR10(root=data_path, train=train, download=download, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n",
    "\n",
    "    loader_params = dict(\n",
    "        shuffle=kwargs.get(\"shuffle\", True),\n",
    "        drop_last=kwargs.get(\"drop_last\", True),\n",
    "        pin_memory=kwargs.get(\"pin_memory\", True),\n",
    "        num_workers=kwargs.get(\"num_workers\", 4),\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, **loader_params)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983954f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.718862Z",
     "iopub.status.busy": "2024-09-06T19:46:24.718562Z",
     "iopub.status.idle": "2024-09-06T19:46:24.734862Z",
     "shell.execute_reply": "2024-09-06T19:46:24.733897Z"
    },
    "papermill": {
     "duration": 0.025908,
     "end_time": "2024-09-06T19:46:24.736973",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.711065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# util.tools\n",
    "\n",
    "def train_one_epoch(trainer, loader, optimizer, device, epoch):\n",
    "    trainer.train()\n",
    "    total_loss, total_num = 0., 0\n",
    "\n",
    "    with tqdm(loader, dynamic_ncols=True, colour=\"#ff924a\") as data:\n",
    "        for images, _ in data:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_0 = images.to(device)\n",
    "            loss = trainer(x_0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_num += x_0.shape[0]\n",
    "\n",
    "            data.set_description(f\"Epoch: {epoch}\")\n",
    "            data.set_postfix(ordered_dict={\n",
    "                \"train_loss\": total_loss / total_num,\n",
    "            })\n",
    "\n",
    "    return total_loss / total_num\n",
    "\n",
    "\n",
    "def save_image(images: torch.Tensor, nrow: int = 8, show: bool = True, path: Optional[str] = None,\n",
    "               format: Optional[str] = None, to_grayscale: bool = False, **kwargs):\n",
    "    \"\"\"\n",
    "    concat all image into a picture.\n",
    "\n",
    "    Parameters:\n",
    "        images: a tensor with shape (batch_size, channels, height, width).\n",
    "        nrow: decide how many images per row. Default `8`.\n",
    "        show: whether to display the image after stitching. Default `True`.\n",
    "        path: the path to save the image. if None (default), will not save image.\n",
    "        format: image format. You can print the set of available formats by running `python3 -m PIL`.\n",
    "        to_grayscale: convert PIL image to grayscale version of image. Default `False`.\n",
    "        **kwargs: other arguments for `torchvision.utils.make_grid`.\n",
    "\n",
    "    Returns:\n",
    "        concat image, a tensor with shape (height, width, channels).\n",
    "    \"\"\"\n",
    "    images = images * 0.5 + 0.5\n",
    "    grid = make_grid(images, nrow=nrow, **kwargs)  # (channels, height, width)\n",
    "    #  (height, width, channels)\n",
    "    grid = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "\n",
    "    im = Image.fromarray(grid)\n",
    "    if to_grayscale:\n",
    "        im = im.convert(mode=\"L\")\n",
    "    if path is not None:\n",
    "        im.save(path, format=format)\n",
    "    if show:\n",
    "        im.show()\n",
    "    return grid\n",
    "\n",
    "\n",
    "def save_sample_image(images: torch.Tensor, show: bool = True, path: Optional[str] = None,\n",
    "                      format: Optional[str] = None, to_grayscale: bool = False, **kwargs):\n",
    "    \"\"\"\n",
    "    concat all image including intermediate process into a picture.\n",
    "\n",
    "    Parameters:\n",
    "        images: images including intermediate process,\n",
    "            a tensor with shape (batch_size, sample, channels, height, width).\n",
    "        show: whether to display the image after stitching. Default `True`.\n",
    "        path: the path to save the image. if None (default), will not save image.\n",
    "        format: image format. You can print the set of available formats by running `python3 -m PIL`.\n",
    "        to_grayscale: convert PIL image to grayscale version of image. Default `False`.\n",
    "        **kwargs: other arguments for `torchvision.utils.make_grid`.\n",
    "\n",
    "    Returns:\n",
    "        concat image, a tensor with shape (height, width, channels).\n",
    "    \"\"\"\n",
    "    images = images * 0.5 + 0.5\n",
    "\n",
    "    grid = []\n",
    "    for i in range(images.shape[0]):\n",
    "        # for each sample in batch, concat all intermediate process images in a row\n",
    "        t = make_grid(images[i], nrow=images.shape[1], **kwargs)  # (channels, height, width)\n",
    "        grid.append(t)\n",
    "    # stack all merged images to a tensor\n",
    "    grid = torch.stack(grid, dim=0)  # (batch_size, channels, height, width)\n",
    "    grid = make_grid(grid, nrow=1, **kwargs)  # concat all batch images in a different row, (channels, height, width)\n",
    "    #  (height, width, channels)\n",
    "    grid = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "\n",
    "    im = Image.fromarray(grid)\n",
    "    if to_grayscale:\n",
    "        im = im.convert(mode=\"L\")\n",
    "    if path is not None:\n",
    "        im.save(path, format=format)\n",
    "    if show:\n",
    "        im.show()\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8076d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.751236Z",
     "iopub.status.busy": "2024-09-06T19:46:24.750589Z",
     "iopub.status.idle": "2024-09-06T19:46:24.787144Z",
     "shell.execute_reply": "2024-09-06T19:46:24.786354Z"
    },
    "papermill": {
     "duration": 0.04591,
     "end_time": "2024-09-06T19:46:24.789098",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.743188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#util.engine\n",
    "\n",
    "def extract(v, i, shape):\n",
    "    \"\"\"\n",
    "    Get the i-th number in v, and the shape of v is mostly (T, ), the shape of i is mostly (batch_size, ).\n",
    "    equal to [v[index] for index in i]\n",
    "    \"\"\"\n",
    "    out = torch.gather(v, index=i, dim=0)\n",
    "    out = out.to(device=i.device, dtype=torch.float32)\n",
    "\n",
    "    # reshape to (batch_size, 1, 1, 1, 1, ...) for broadcasting purposes.\n",
    "    out = out.view([i.shape[0]] + [1] * (len(shape) - 1))\n",
    "    return out\n",
    "\n",
    "\n",
    "class GaussianDiffusionTrainer(nn.Module):\n",
    "    def __init__(self, model: nn.Module, beta: Tuple[int, int], T: int):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.T = T\n",
    "\n",
    "        # generate T steps of beta\n",
    "        self.register_buffer(\"beta_t\", torch.linspace(*beta, T, dtype=torch.float32))\n",
    "\n",
    "        # calculate the cumulative product of $\\alpha$ , named $\\bar{\\alpha_t}$ in paper\n",
    "        alpha_t = 1.0 - self.beta_t\n",
    "        alpha_t_bar = torch.cumprod(alpha_t, dim=0)\n",
    "\n",
    "        # calculate and store two coefficient of $q(x_t | x_0)$\n",
    "        self.register_buffer(\"signal_rate\", torch.sqrt(alpha_t_bar))\n",
    "        self.register_buffer(\"noise_rate\", torch.sqrt(1.0 - alpha_t_bar))\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        # get a random training step $t \\sim Uniform({1, ..., T})$\n",
    "        t = torch.randint(self.T, size=(x_0.shape[0],), device=x_0.device)\n",
    "\n",
    "        # generate $\\epsilon \\sim N(0, 1)$\n",
    "        epsilon = torch.randn_like(x_0)\n",
    "\n",
    "        # predict the noise added from $x_{t-1}$ to $x_t$\n",
    "        x_t = (extract(self.signal_rate, t, x_0.shape) * x_0 +\n",
    "               extract(self.noise_rate, t, x_0.shape) * epsilon)\n",
    "        epsilon_theta = self.model(x_t, t)\n",
    "\n",
    "        # get the gradient\n",
    "        loss = F.mse_loss(epsilon_theta, epsilon, reduction=\"none\")\n",
    "        loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class DDPMSampler(nn.Module):\n",
    "    def __init__(self, model: nn.Module, beta: Tuple[int, int], T: int):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.T = T\n",
    "\n",
    "        # generate T steps of beta\n",
    "        self.register_buffer(\"beta_t\", torch.linspace(*beta, T, dtype=torch.float32))\n",
    "\n",
    "        # calculate the cumulative product of $\\alpha$ , named $\\bar{\\alpha_t}$ in paper\n",
    "        alpha_t = 1.0 - self.beta_t\n",
    "        alpha_t_bar = torch.cumprod(alpha_t, dim=0)\n",
    "        alpha_t_bar_prev = F.pad(alpha_t_bar[:-1], (1, 0), value=1.0)\n",
    "\n",
    "        self.register_buffer(\"coeff_1\", torch.sqrt(1.0 / alpha_t))\n",
    "        self.register_buffer(\"coeff_2\", self.coeff_1 * (1.0 - alpha_t) / torch.sqrt(1.0 - alpha_t_bar))\n",
    "        self.register_buffer(\"posterior_variance\", self.beta_t * (1.0 - alpha_t_bar_prev) / (1.0 - alpha_t_bar))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def cal_mean_variance(self, x_t, t):\n",
    "        \"\"\"\n",
    "        Calculate the mean and variance for $q(x_{t-1} | x_t, x_0)$\n",
    "        \"\"\"\n",
    "        epsilon_theta = self.model(x_t, t)\n",
    "        mean = extract(self.coeff_1, t, x_t.shape) * x_t - extract(self.coeff_2, t, x_t.shape) * epsilon_theta\n",
    "\n",
    "        # var is a constant\n",
    "        var = extract(self.posterior_variance, t, x_t.shape)\n",
    "\n",
    "        return mean, var\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_one_step(self, x_t, time_step: int):\n",
    "        \"\"\"\n",
    "        Calculate $x_{t-1}$ according to $x_t$\n",
    "        \"\"\"\n",
    "        t = torch.full((x_t.shape[0],), time_step, device=x_t.device, dtype=torch.long)\n",
    "        mean, var = self.cal_mean_variance(x_t, t)\n",
    "\n",
    "        z = torch.randn_like(x_t) if time_step > 0 else 0\n",
    "        x_t_minus_one = mean + torch.sqrt(var) * z\n",
    "\n",
    "        if torch.isnan(x_t_minus_one).int().sum() != 0:\n",
    "            raise ValueError(\"nan in tensor!\")\n",
    "\n",
    "        return x_t_minus_one\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x_t, only_return_x_0: bool = True, interval: int = 1, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x_t: Standard Gaussian noise. A tensor with shape (batch_size, channels, height, width).\n",
    "            only_return_x_0: Determines whether the image is saved during the sampling process. if True,\n",
    "                intermediate pictures are not saved, and only return the final result $x_0$.\n",
    "            interval: This parameter is valid only when `only_return_x_0 = False`. Decide the interval at which\n",
    "                to save the intermediate process pictures.\n",
    "                $x_t$ and $x_0$ will be included, no matter what the value of `interval` is.\n",
    "            kwargs: no meaning, just for compatibility.\n",
    "\n",
    "        Returns:\n",
    "            if `only_return_x_0 = True`, will return a tensor with shape (batch_size, channels, height, width),\n",
    "            otherwise, return a tensor with shape (batch_size, sample, channels, height, width),\n",
    "            include intermediate pictures.\n",
    "        \"\"\"\n",
    "        x = [x_t]\n",
    "        with tqdm(reversed(range(self.T)), colour=\"#6565b5\", total=self.T) as sampling_steps:\n",
    "            for time_step in sampling_steps:\n",
    "                x_t = self.sample_one_step(x_t, time_step)\n",
    "\n",
    "                if not only_return_x_0 and ((self.T - time_step) % interval == 0 or time_step == 0):\n",
    "                    x.append(torch.clip(x_t, -1.0, 1.0))\n",
    "\n",
    "                sampling_steps.set_postfix(ordered_dict={\"step\": time_step + 1, \"sample\": len(x)})\n",
    "\n",
    "        if only_return_x_0:\n",
    "            return x_t  # [batch_size, channels, height, width]\n",
    "        return torch.stack(x, dim=1)  # [batch_size, sample, channels, height, width]\n",
    "\n",
    "\n",
    "class DDIMSampler(nn.Module):\n",
    "    def __init__(self, model, beta: Tuple[int, int], T: int):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.T = T\n",
    "\n",
    "        # generate T steps of beta\n",
    "        beta_t = torch.linspace(*beta, T, dtype=torch.float32)\n",
    "        # calculate the cumulative product of $\\alpha$ , named $\\bar{\\alpha_t}$ in paper\n",
    "        alpha_t = 1.0 - beta_t\n",
    "        self.register_buffer(\"alpha_t_bar\", torch.cumprod(alpha_t, dim=0))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_one_step(self, x_t, time_step: int, prev_time_step: int, eta: float):\n",
    "        t = torch.full((x_t.shape[0],), time_step, device=x_t.device, dtype=torch.long)\n",
    "        prev_t = torch.full((x_t.shape[0],), prev_time_step, device=x_t.device, dtype=torch.long)\n",
    "\n",
    "        # get current and previous alpha_cumprod\n",
    "        alpha_t = extract(self.alpha_t_bar, t, x_t.shape)\n",
    "        alpha_t_prev = extract(self.alpha_t_bar, prev_t, x_t.shape)\n",
    "\n",
    "        # predict noise using model\n",
    "        epsilon_theta_t = self.model(x_t, t)\n",
    "\n",
    "        # calculate x_{t-1}\n",
    "        sigma_t = eta * torch.sqrt((1 - alpha_t_prev) / (1 - alpha_t) * (1 - alpha_t / alpha_t_prev))\n",
    "        epsilon_t = torch.randn_like(x_t)\n",
    "        x_t_minus_one = (\n",
    "                torch.sqrt(alpha_t_prev / alpha_t) * x_t +\n",
    "                (torch.sqrt(1 - alpha_t_prev - sigma_t ** 2) - torch.sqrt(\n",
    "                    (alpha_t_prev * (1 - alpha_t)) / alpha_t)) * epsilon_theta_t +\n",
    "                sigma_t * epsilon_t\n",
    "        )\n",
    "        return x_t_minus_one\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x_t, steps: int = 1, method=\"linear\", eta=0.0,\n",
    "                only_return_x_0: bool = True, interval: int = 1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x_t: Standard Gaussian noise. A tensor with shape (batch_size, channels, height, width).\n",
    "            steps: Sampling steps.\n",
    "            method: Sampling method, can be \"linear\" or \"quadratic\".\n",
    "            eta: Coefficients of sigma parameters in the paper. The value 0 indicates DDIM, 1 indicates DDPM.\n",
    "            only_return_x_0: Determines whether the image is saved during the sampling process. if True,\n",
    "                intermediate pictures are not saved, and only return the final result $x_0$.\n",
    "            interval: This parameter is valid only when `only_return_x_0 = False`. Decide the interval at which\n",
    "                to save the intermediate process pictures, according to `step`.\n",
    "                $x_t$ and $x_0$ will be included, no matter what the value of `interval` is.\n",
    "\n",
    "        Returns:\n",
    "            if `only_return_x_0 = True`, will return a tensor with shape (batch_size, channels, height, width),\n",
    "            otherwise, return a tensor with shape (batch_size, sample, channels, height, width),\n",
    "            include intermediate pictures.\n",
    "        \"\"\"\n",
    "        if method == \"linear\":\n",
    "            a = self.T // steps\n",
    "            time_steps = np.asarray(list(range(0, self.T, a)))\n",
    "        elif method == \"quadratic\":\n",
    "            time_steps = (np.linspace(0, np.sqrt(self.T * 0.8), steps) ** 2).astype(np.int)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"sampling method {method} is not implemented!\")\n",
    "\n",
    "        # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "        time_steps = time_steps + 1\n",
    "        # previous sequence\n",
    "        time_steps_prev = np.concatenate([[0], time_steps[:-1]])\n",
    "\n",
    "        x = [x_t]\n",
    "        with tqdm(reversed(range(0, steps)), colour=\"#6565b5\", total=steps) as sampling_steps:\n",
    "            for i in sampling_steps:\n",
    "                x_t = self.sample_one_step(x_t, time_steps[i], time_steps_prev[i], eta)\n",
    "\n",
    "                if not only_return_x_0 and ((steps - i) % interval == 0 or i == 0):\n",
    "                    x.append(torch.clip(x_t, -1.0, 1.0))\n",
    "\n",
    "                sampling_steps.set_postfix(ordered_dict={\"step\": i + 1, \"sample\": len(x)})\n",
    "\n",
    "        if only_return_x_0:\n",
    "            return x_t  # [batch_size, channels, height, width]\n",
    "        return torch.stack(x, dim=1)  # [batch_size, sample, channels, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e72337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.803298Z",
     "iopub.status.busy": "2024-09-06T19:46:24.802980Z",
     "iopub.status.idle": "2024-09-06T19:46:24.826205Z",
     "shell.execute_reply": "2024-09-06T19:46:24.825420Z"
    },
    "papermill": {
     "duration": 0.032575,
     "end_time": "2024-09-06T19:46:24.828159",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.795584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#util.callbacks\n",
    "\n",
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, monitor: str = 'val_loss', mode: str = 'min', patience: int = 1):\n",
    "        \"\"\"\n",
    "        Decide whether to terminate the training early according to the indicators to be monitored\n",
    "        and the number of times, and terminate early when the monitoring exceeds the specified number of\n",
    "        consecutive times without better.\n",
    "\n",
    "        Parameters:\n",
    "            monitor: the indicators to be monitored will only take effect if they are passed a `dict`.\n",
    "            mode: mode for monitoring indicators, 'min' or 'max'.\n",
    "            patience: maximum tolerance times.\n",
    "        \"\"\"\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.__value = -math.inf if mode == 'max' else math.inf\n",
    "        self.__times = 0\n",
    "\n",
    "    def state_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        save state for next load recovery.\n",
    "\n",
    "        for example:\n",
    "            ```\n",
    "            torch.save(state_dict, path)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'monitor': self.monitor,\n",
    "            'mode': self.mode,\n",
    "            'patience': self.patience,\n",
    "            'value': self.__value,\n",
    "            'times': self.__times\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict):\n",
    "        \"\"\"\n",
    "        load state\n",
    "        \"\"\"\n",
    "        self.monitor = state_dict['monitor']\n",
    "        self.mode = state_dict['mode']\n",
    "        self.patience = state_dict['patience']\n",
    "        self.__value = state_dict['value']\n",
    "        self.__times = state_dict['times']\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset tolerance times\n",
    "        \"\"\"\n",
    "        self.__times = 0\n",
    "\n",
    "    def step(self, metrics: Union[Dict, int, float]) -> bool:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            metrics: dict contains `monitor` or a scalar\n",
    "\n",
    "        Returns:\n",
    "            bool. Returns True if early termination is required, otherwise returns False\n",
    "        \"\"\"\n",
    "        if isinstance(metrics, dict):\n",
    "            metrics = metrics[self.monitor]\n",
    "\n",
    "        if (self.mode == 'min' and metrics <= self.__value) or (\n",
    "                self.mode == 'max' and metrics >= self.__value):\n",
    "            self.__value = metrics\n",
    "            self.__times = 0\n",
    "        else:\n",
    "            self.__times += 1\n",
    "        if self.__times >= self.patience:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, filepath: str = 'checkpoint.pth', monitor: str = 'val_loss',\n",
    "                 mode: str = 'min', save_best_only: bool = False, save_freq: int = 1):\n",
    "        \"\"\"\n",
    "        auto save checkpoint during training.\n",
    "\n",
    "        Parameters:\n",
    "            filepath: File name or folder name, the location where it needs to be saved,\n",
    "                or in the case of a folder, the number of checkpoints saved may be more than one.\n",
    "            monitor: the indicators to be monitored will only take effect if they are passed a `dict`.\n",
    "            mode: mode for monitoring indicators, 'min' or 'max'.\n",
    "            save_best_only: whether to save only the checkpoints with the best indicators.\n",
    "            save_freq: frequency of saving, only valid if `save_best_only=False`.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.save_best_only = save_best_only\n",
    "        self.mode = mode\n",
    "        self.save_freq = save_freq\n",
    "        self.__times = 1\n",
    "        self.__value = -math.inf if mode == 'max' else math.inf\n",
    "\n",
    "    @staticmethod\n",
    "    def save(filepath: str, times: int = None, **kwargs):\n",
    "        \"\"\"\n",
    "        save checkpoint.\n",
    "\n",
    "        Parameters:\n",
    "            filepath: File name or folder name, the location where it needs to be saved,\n",
    "                or in the case of a folder, the number of checkpoints saved may be more than one.\n",
    "            times: number of current saves, used only if the save path is a folder, used only for naming files.\n",
    "            kwargs: all content to be saved.\n",
    "        \"\"\"\n",
    "        path = Path(filepath)\n",
    "#         if path.is_dir():\n",
    "#             if not path.exists():\n",
    "#                 path.mkdir(parents=True)\n",
    "#             path.joinpath(f'checkpoint-{times}.pth')\n",
    "        x=f'checkpoint-{times}.pth'\n",
    "        os.makedirs(os.path.dirname(os.path.join(filepath,x)), exist_ok=True)\n",
    "        torch.save(kwargs,os.path.join(filepath,x))\n",
    "\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"\n",
    "        save state for next load recovery.\n",
    "\n",
    "        for example:\n",
    "            ```\n",
    "            torch.save(state_dict, path)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'filepath': self.filepath,\n",
    "            'monitor': self.monitor,\n",
    "            'save_best_only': self.save_best_only,\n",
    "            'mode': self.mode,\n",
    "            'save_freq': self.save_freq,\n",
    "            'times': self.__times,\n",
    "            'value': self.__value\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict):\n",
    "        \"\"\"\n",
    "        load state\n",
    "        \"\"\"\n",
    "        self.filepath = state_dict['filepath']\n",
    "        self.monitor = state_dict['monitor']\n",
    "        self.save_best_only = state_dict['save_best_only']\n",
    "        self.mode = state_dict['mode']\n",
    "        self.save_freq = state_dict['save_freq']\n",
    "        self.__times = state_dict['times']\n",
    "        self.__value = state_dict['value']\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset count times\n",
    "        \"\"\"\n",
    "        self.__times = 1\n",
    "\n",
    "    def step(self, metrics: Union[Dict, int, float], **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            metrics: dict contains `monitor` or a scalar\n",
    "            kwargs: all content to be saved.\n",
    "        \"\"\"\n",
    "        if isinstance(metrics, dict):\n",
    "            metrics = metrics[self.monitor]\n",
    "\n",
    "        flag = False\n",
    "\n",
    "        if self.save_best_only:\n",
    "            if (self.mode == 'min' and metrics <= self.__value) or (\n",
    "                    self.mode == 'max' and metrics >= self.__value):\n",
    "                self.__value = metrics\n",
    "                self.save(self.filepath, self.__times, **kwargs)\n",
    "                flag = True\n",
    "        else:\n",
    "            if self.__times % self.save_freq == 0:\n",
    "                self.save(self.filepath, self.__times, **kwargs)\n",
    "                flag = True\n",
    "\n",
    "        self.__times += 1\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0be79e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T19:46:24.841845Z",
     "iopub.status.busy": "2024-09-06T19:46:24.841538Z",
     "iopub.status.idle": "2024-09-06T20:51:12.242616Z",
     "shell.execute_reply": "2024-09-06T20:51:12.241325Z"
    },
    "papermill": {
     "duration": 3889.654792,
     "end_time": "2024-09-06T20:51:14.489154",
     "exception": false,
     "start_time": "2024-09-06T19:46:24.834362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 170498071/170498071 [00:11<00:00, 14620125.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:14<00:00,  4.02it/s, train_loss=179]\n",
      "Epoch: 2: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.05it/s, train_loss=119]\n",
      "Epoch: 3: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:13<00:00,  4.05it/s, train_loss=110]\n",
      "Epoch: 4: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:13<00:00,  4.04it/s, train_loss=105]\n",
      "Epoch: 5: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:13<00:00,  4.05it/s, train_loss=104]\n",
      "Epoch: 6: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:13<00:00,  4.04it/s, train_loss=105]\n",
      "Epoch: 7: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:13<00:00,  4.04it/s, train_loss=104]\n",
      "Epoch: 8: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.05it/s, train_loss=101]\n",
      "Epoch: 9: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:13<00:00,  4.05it/s, train_loss=100]\n",
      "Epoch: 10: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.05it/s, train_loss=99.8]\n",
      "Epoch: 11: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:13<00:00,  4.05it/s, train_loss=98.5]\n",
      "Epoch: 12: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.05it/s, train_loss=99.5]\n",
      "Epoch: 13: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.06it/s, train_loss=99]\n",
      "Epoch: 14: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.06it/s, train_loss=98.8]\n",
      "Epoch: 15: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.06it/s, train_loss=97.1]\n",
      "Epoch: 16: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.07it/s, train_loss=97.4]\n",
      "Epoch: 17: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.07it/s, train_loss=96.7]\n",
      "Epoch: 18: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.07it/s, train_loss=95.2]\n",
      "Epoch: 19: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.07it/s, train_loss=95.9]\n",
      "Epoch: 20: 100%|\u001b[38;2;255;146;74m\u001b[0m| 781/781 [03:12<00:00,  4.05it/s, train_loss=97.8]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "def train(config):\n",
    "    consume = config[\"consume\"]\n",
    "#     if consume:\n",
    "#         cp = torch.load(config[\"consume_path\"])\n",
    "#         config = cp[\"config\"]\n",
    "#     print(config)\n",
    "\n",
    "    device = torch.device(config[\"device\"])\n",
    "#     print(\"hehe\")\n",
    "    loader = create_cifar10_dataset(**config[\"Dataset\"])\n",
    "#     print(\"hehe1\")\n",
    "    start_epoch = 1\n",
    "\n",
    "    model = UNet(**config[\"Model\"]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-4)\n",
    "    trainer = GaussianDiffusionTrainer(model, **config[\"Trainer\"]).to(device)\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(**config[\"Callback\"])\n",
    "\n",
    "    if consume:\n",
    "        model.load_state_dict(cp[\"model\"])\n",
    "        optimizer.load_state_dict(cp[\"optimizer\"])\n",
    "        model_checkpoint.load_state_dict(cp[\"model_checkpoint\"])\n",
    "        start_epoch = cp[\"start_epoch\"] + 1\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"epochs\"] + 1):\n",
    "        loss = train_one_epoch(trainer, loader, optimizer, device, epoch)\n",
    "        model_checkpoint.step(loss, model=model.state_dict(), config=config,\n",
    "                              optimizer=optimizer.state_dict(), start_epoch=epoch,\n",
    "                              model_checkpoint=model_checkpoint.state_dict())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     config = load_yaml(\"config.yml\", encoding=\"utf-8\")\n",
    "    train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e48981c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T20:51:22.310682Z",
     "iopub.status.busy": "2024-09-06T20:51:22.310209Z",
     "iopub.status.idle": "2024-09-06T20:51:22.315829Z",
     "shell.execute_reply": "2024-09-06T20:51:22.314890Z"
    },
    "papermill": {
     "duration": 3.902486,
     "end_time": "2024-09-06T20:51:22.318079",
     "exception": false,
     "start_time": "2024-09-06T20:51:18.415593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': {'in_channels': 3, 'out_channels': 3, 'model_channels': 128, 'attention_resolutions': [2], 'num_res_blocks': 2, 'dropout': 0.1, 'channel_mult': [1, 2, 2, 2], 'conv_resample': True, 'num_heads': 4}, 'Dataset': {'dataset': 'cifar', 'train': True, 'data_path': './data', 'download': True, 'image_size': [32, 32], 'mode': 'RGB', 'suffix': ['png', 'jpg'], 'batch_size': 64, 'shuffle': True, 'drop_last': True, 'pin_memory': True, 'num_workers': 4}, 'Trainer': {'T': 1000, 'beta': [0.0001, 0.02]}, 'Callback': {'filepath': '/kaggle/working/checkpoint/cifar10.pth', 'save_freq': 1}, 'device': 'cuda:0', 'epochs': 20, 'consume': False, 'consume_path': '/kaggle/working/checkpoint/cifar10.pth', 'lr': 0.0002}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf1f3fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T20:51:29.991778Z",
     "iopub.status.busy": "2024-09-06T20:51:29.990852Z",
     "iopub.status.idle": "2024-09-06T20:51:29.998083Z",
     "shell.execute_reply": "2024-09-06T20:51:29.997244Z"
    },
    "papermill": {
     "duration": 3.879485,
     "end_time": "2024-09-06T20:51:29.999985",
     "exception": false,
     "start_time": "2024-09-06T20:51:26.120500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"consume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45ebfb",
   "metadata": {
    "papermill": {
     "duration": 3.85954,
     "end_time": "2024-09-06T20:51:37.711708",
     "exception": false,
     "start_time": "2024-09-06T20:51:33.852168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3968.32581,
   "end_time": "2024-09-06T20:51:43.099182",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-06T19:45:34.773372",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
