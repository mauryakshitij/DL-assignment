{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9326288,"sourceType":"datasetVersion","datasetId":5650148}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-06T01:21:39.665676Z","iopub.execute_input":"2024-09-06T01:21:39.665974Z","iopub.status.idle":"2024-09-06T01:21:39.670677Z","shell.execute_reply.started":"2024-09-06T01:21:39.665942Z","shell.execute_reply":"2024-09-06T01:21:39.6697Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#imports\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\nimport glob\nimport os\nfrom torch.optim import Adam\n\nimport torchvision\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset\n\nimport argparse\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm\n# from models.unet_base import Unet\n# from scheduler.linear_noise_scheduler import LinearNoiseScheduler\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T10:04:11.612097Z","iopub.execute_input":"2024-09-06T10:04:11.612477Z","iopub.status.idle":"2024-09-06T10:04:12.808882Z","shell.execute_reply.started":"2024-09-06T10:04:11.612439Z","shell.execute_reply":"2024-09-06T10:04:12.808044Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    \"dataset_params\": {\n        \"im_path\": '/kaggle/input/ddpm-mnist/train/images'\n    },\n    \"diffusion_params\": {\n        \"num_timesteps\": 1000,\n        \"beta_start\": 0.0001,\n        \"beta_end\": 0.02\n    },\n    \"model_params\": {\n        \"im_channels\": 1,\n        \"im_size\": 28,\n        \"down_channels\": [32, 64, 128, 256],\n        \"mid_channels\": [256, 256, 128],\n        \"down_sample\": [True, True, False],\n        \"time_emb_dim\": 128,\n        \"num_down_layers\": 2,\n        \"num_mid_layers\": 2,\n        \"num_up_layers\": 2,\n        \"num_heads\": 4\n    },\n    \"train_params\": {\n        \"task_name\": 'default',\n        \"batch_size\": 64,\n        \"num_epochs\": 20,\n        \"num_samples\": 100,\n        \"num_grid_rows\": 10,\n        \"lr\": 0.0001,\n        \"ckpt_name\": 'ddpm_ckpt.pth'\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-06T10:04:12.81034Z","iopub.execute_input":"2024-09-06T10:04:12.810753Z","iopub.status.idle":"2024-09-06T10:04:12.817117Z","shell.execute_reply.started":"2024-09-06T10:04:12.810718Z","shell.execute_reply":"2024-09-06T10:04:12.816278Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Linear Noise Scheduler\n\nclass LinearNoiseScheduler:\n    r\"\"\"\n    Class for the linear noise scheduler that is used in DDPM.\n    \"\"\"\n    def __init__(self, num_timesteps, beta_start, beta_end):\n        self.num_timesteps = num_timesteps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        \n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n        self.alphas = 1. - self.betas\n        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n        \n    def add_noise(self, original, noise, t):\n        r\"\"\"\n        Forward method for diffusion\n        :param original: Image on which noise is to be applied\n        :param noise: Random Noise Tensor (from normal dist)\n        :param t: timestep of the forward process of shape -> (B,)\n        :return:\n        \"\"\"\n        original_shape = original.shape\n        batch_size = original_shape[0]\n        \n        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n        \n        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n        for _ in range(len(original_shape) - 1):\n            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n        for _ in range(len(original_shape) - 1):\n            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n        \n        # Apply and Return Forward process equation\n        return (sqrt_alpha_cum_prod.to(original.device) * original\n                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n        \n    def sample_prev_timestep(self, xt, noise_pred, t):\n        r\"\"\"\n            Use the noise prediction by model to get\n            xt-1 using xt and the noise predicted\n        :param xt: current timestep sample\n        :param noise_pred: model noise prediction\n        :param t: current timestep we are at\n        :return:\n        \"\"\"\n        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n        x0 = torch.clamp(x0, -1., 1.)\n        \n        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n        \n        if t == 0:\n            return mean, x0\n        else:\n            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n            variance = variance * self.betas.to(xt.device)[t]\n            sigma = variance ** 0.5\n            z = torch.randn(xt.shape).to(xt.device)\n            \n            # OR\n            # variance = self.betas[t]\n            # sigma = variance ** 0.5\n            # z = torch.randn(xt.shape).to(xt.device)\n            return mean + sigma * z, x0\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T10:04:12.997857Z","iopub.execute_input":"2024-09-06T10:04:12.998229Z","iopub.status.idle":"2024-09-06T10:04:13.01259Z","shell.execute_reply.started":"2024-09-06T10:04:12.998192Z","shell.execute_reply":"2024-09-06T10:04:13.01162Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Unet Base\n\ndef get_time_embedding(time_steps, temb_dim):\n    r\"\"\"\n    Convert time steps tensor into an embedding using the\n    sinusoidal time embedding formula\n    :param time_steps: 1D tensor of length batch size\n    :param temb_dim: Dimension of the embedding\n    :return: BxD embedding representation of B time steps\n    \"\"\"\n    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n    \n    # factor = 10000^(2i/d_model)\n    factor = 10000 ** ((torch.arange(\n        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n    )\n    \n    # pos / factor\n    # timesteps B -> B, 1 -> B, temb_dim\n    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n    return t_emb\n\n\nclass DownBlock(nn.Module):\n    r\"\"\"\n    Down conv block with attention.\n    Sequence of following block\n    1. Resnet block with time embedding\n    2. Attention block\n    3. Downsample using 2x2 average pooling\n    \"\"\"\n    def __init__(self, in_channels, out_channels, t_emb_dim,\n                 down_sample=True, num_heads=4, num_layers=1):\n        super().__init__()\n        self.num_layers = num_layers\n        self.down_sample = down_sample\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        self.t_emb_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(t_emb_dim, out_channels)\n            )\n            for _ in range(num_layers)\n        ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        self.attention_norms = nn.ModuleList(\n            [nn.GroupNorm(8, out_channels)\n             for _ in range(num_layers)]\n        )\n        \n        self.attentions = nn.ModuleList(\n            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n             for _ in range(num_layers)]\n        )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n                                          4, 2, 1) if self.down_sample else nn.Identity()\n    \n    def forward(self, x, t_emb):\n        out = x\n        for i in range(self.num_layers):\n            \n            # Resnet block of Unet\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            \n            # Attention block of Unet\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n            \n        out = self.down_sample_conv(out)\n        return out\n\n\nclass MidBlock(nn.Module):\n    r\"\"\"\n    Mid conv block with attention.\n    Sequence of following blocks\n    1. Resnet block with time embedding\n    2. Attention block\n    3. Resnet block with time embedding\n    \"\"\"\n    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n        super().__init__()\n        self.num_layers = num_layers\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers+1)\n            ]\n        )\n        self.t_emb_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(t_emb_dim, out_channels)\n            )\n            for _ in range(num_layers + 1)\n        ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers+1)\n            ]\n        )\n        \n        self.attention_norms = nn.ModuleList(\n            [nn.GroupNorm(8, out_channels)\n                for _ in range(num_layers)]\n        )\n        \n        self.attentions = nn.ModuleList(\n            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)]\n        )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers+1)\n            ]\n        )\n    \n    def forward(self, x, t_emb):\n        out = x\n        \n        # First resnet block\n        resnet_input = out\n        out = self.resnet_conv_first[0](out)\n        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n        out = self.resnet_conv_second[0](out)\n        out = out + self.residual_input_conv[0](resnet_input)\n        \n        for i in range(self.num_layers):\n            \n            # Attention Block\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n            \n            # Resnet Block\n            resnet_input = out\n            out = self.resnet_conv_first[i+1](out)\n            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i+1](out)\n            out = out + self.residual_input_conv[i+1](resnet_input)\n        \n        return out\n\n\nclass UpBlock(nn.Module):\n    r\"\"\"\n    Up conv block with attention.\n    Sequence of following blocks\n    1. Upsample\n    1. Concatenate Down block output\n    2. Resnet block with time embedding\n    3. Attention Block\n    \"\"\"\n    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n        super().__init__()\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        self.t_emb_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(t_emb_dim, out_channels)\n            )\n            for _ in range(num_layers)\n        ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        \n        self.attention_norms = nn.ModuleList(\n            [\n                nn.GroupNorm(8, out_channels)\n                for _ in range(num_layers)\n            ]\n        )\n        \n        self.attentions = nn.ModuleList(\n            [\n                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)\n            ]\n        )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n                                                 4, 2, 1) \\\n            if self.up_sample else nn.Identity()\n    \n    def forward(self, x, out_down, t_emb):\n        x = self.up_sample_conv(x)\n        x = torch.cat([x, out_down], dim=1)\n        \n        out = x\n        for i in range(self.num_layers):\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            \n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n\n        return out\n\n\nclass Unet(nn.Module):\n    r\"\"\"\n    Unet model comprising\n    Down blocks, Midblocks and Uplocks\n    \"\"\"\n    def __init__(self, model_config):\n        super().__init__()\n        im_channels = model_config['im_channels']\n        self.down_channels = model_config['down_channels']\n        self.mid_channels = model_config['mid_channels']\n        self.t_emb_dim = model_config['time_emb_dim']\n        self.down_sample = model_config['down_sample']\n        self.num_down_layers = model_config['num_down_layers']\n        self.num_mid_layers = model_config['num_mid_layers']\n        self.num_up_layers = model_config['num_up_layers']\n        \n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-2]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        \n        # Initial projection from sinusoidal time embedding\n        self.t_proj = nn.Sequential(\n            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n            nn.SiLU(),\n            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n        )\n\n        self.up_sample = list(reversed(self.down_sample))\n        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n        \n        self.downs = nn.ModuleList([])\n        for i in range(len(self.down_channels)-1):\n            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers))\n        \n        self.mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels)-1):\n            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n                                      num_layers=self.num_mid_layers))\n        \n        self.ups = nn.ModuleList([])\n        for i in reversed(range(len(self.down_channels)-1)):\n            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n                                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers))\n        \n        self.norm_out = nn.GroupNorm(8, 16)\n        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n    \n    def forward(self, x, t):\n        # Shapes assuming downblocks are [C1, C2, C3, C4]\n        # Shapes assuming midblocks are [C4, C4, C3]\n        # Shapes assuming downsamples are [True, True, False]\n        # B x C x H x W\n        out = self.conv_in(x)\n        # B x C1 x H x W\n        \n        # t_emb -> B x t_emb_dim\n        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n        t_emb = self.t_proj(t_emb)\n        \n        down_outs = []\n        \n        for idx, down in enumerate(self.downs):\n            down_outs.append(out)\n            out = down(out, t_emb)\n        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n        # out B x C4 x H/4 x W/4\n            \n        for mid in self.mids:\n            out = mid(out, t_emb)\n        # out B x C3 x H/4 x W/4\n        \n        for up in self.ups:\n            down_out = down_outs.pop()\n            out = up(out, down_out, t_emb)\n            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n        out = self.norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.conv_out(out)\n        # out B x C x H x W\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-09-06T10:04:13.734128Z","iopub.execute_input":"2024-09-06T10:04:13.734477Z","iopub.status.idle":"2024-09-06T10:04:13.789945Z","shell.execute_reply.started":"2024-09-06T10:04:13.734444Z","shell.execute_reply":"2024-09-06T10:04:13.788973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MnistDataset(Dataset):\n    r\"\"\"\n    Nothing special here. Just a simple dataset class for mnist images.\n    Created a dataset class rather using torchvision to allow\n    replacement with any other image dataset\n    \"\"\"\n    def __init__(self, split, im_path, im_ext='png'):\n        r\"\"\"\n        Init method for initializing the dataset properties\n        :param split: train/test to locate the image files\n        :param im_path: root folder of images\n        :param im_ext: image extension. assumes all\n        images would be this type.\n        \"\"\"\n        self.split = split\n        self.im_ext = im_ext\n        self.images, self.labels = self.load_images(im_path)\n    \n    def load_images(self, im_path):\n        r\"\"\"\n        Gets all images from the path specified\n        and stacks them all up\n        :param im_path:\n        :return:\n        \"\"\"\n        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n        ims = []\n        labels = []\n        for d_name in tqdm(os.listdir(im_path)):\n            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n                ims.append(fname)\n                labels.append(int(d_name))\n        print('Found {} images for split {}'.format(len(ims), self.split))\n        return ims, labels\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        im = Image.open(self.images[index])\n        im_tensor = torchvision.transforms.ToTensor()(im)\n        \n        # Convert input to -1 to 1 range.\n        im_tensor = (2 * im_tensor) - 1\n        return im_tensor","metadata":{"execution":{"iopub.status.busy":"2024-09-06T10:04:14.520126Z","iopub.execute_input":"2024-09-06T10:04:14.521023Z","iopub.status.idle":"2024-09-06T10:04:14.530864Z","shell.execute_reply.started":"2024-09-06T10:04:14.520981Z","shell.execute_reply":"2024-09-06T10:04:14.529839Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train ddpm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef train():\n    \n    \n    diffusion_config = config['diffusion_params']\n    dataset_config = config['dataset_params']\n    model_config = config['model_params']\n    train_config = config['train_params']\n    \n    # Create the noise scheduler\n    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n                                     beta_start=diffusion_config['beta_start'],\n                                     beta_end=diffusion_config['beta_end'])\n    \n    # Create the dataset\n    mnist = MnistDataset('train', im_path=dataset_config['im_path'])\n    mnist_loader = DataLoader(mnist, batch_size=train_config['batch_size'], shuffle=True, num_workers=4)\n    \n    # Instantiate the model\n    model = Unet(model_config).to(device)\n    model.train()\n    \n    # Create output directories\n    if not os.path.exists(train_config['task_name']):\n        os.mkdir(train_config['task_name'])\n    \n    # Load checkpoint if found\n    if os.path.exists(os.path.join(train_config['task_name'],train_config['ckpt_name'])):\n        print('Loading checkpoint as found one')\n        model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n                                                      train_config['ckpt_name']), map_location=device))\n    # Specify training parameters\n    num_epochs = train_config['num_epochs']\n    optimizer = Adam(model.parameters(), lr=train_config['lr'])\n    criterion = torch.nn.MSELoss()\n    \n    # Run training\n    for epoch_idx in range(num_epochs):\n        losses = []\n        for im in tqdm(mnist_loader):\n            optimizer.zero_grad()\n            im = im.float().to(device)\n            \n            # Sample random noise\n            noise = torch.randn_like(im).to(device)\n            \n            # Sample timestep\n            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n            \n            # Add noise to images according to timestep\n            noisy_im = scheduler.add_noise(im, noise, t)\n            noise_pred = model(noisy_im, t)\n\n            loss = criterion(noise_pred, noise)\n            losses.append(loss.item())\n            loss.backward()\n            optimizer.step()\n        print('Finished epoch:{} | Loss : {:.4f}'.format(\n            epoch_idx + 1,\n            np.mean(losses),\n        ))\n        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n                                                    train_config['ckpt_name']))\n    \n    print('Done Training ...')\n    \n\nif __name__ == '__main__':\n#     parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n#     parser.add_argument('--config', dest='config_path',\n#                         default='config/default.yaml', type=str)\n#     args = parser.parse_args()\n    train()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T01:21:41.501607Z","iopub.execute_input":"2024-09-06T01:21:41.50195Z","iopub.status.idle":"2024-09-06T02:30:03.227345Z","shell.execute_reply.started":"2024-09-06T01:21:41.501883Z","shell.execute_reply":"2024-09-06T02:30:03.226072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #sample ddpm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef sample(model, scheduler, train_config, model_config, diffusion_config):\n    r\"\"\"\n    Sample stepwise by going backward one timestep at a time.\n    We save the x0 predictions\n    \"\"\"\n    xt = torch.randn((train_config['num_samples'],\n                      model_config['im_channels'],\n                      model_config['im_size'],\n                      model_config['im_size'])).to(device)\n    for i in tqdm(reversed(range(diffusion_config['num_timesteps']))):\n        # Get prediction of noise\n        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n        \n        # Use scheduler to get x0 and xt-1\n        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n        \n        # Save x0\n        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n        ims = (ims + 1) / 2\n        grid = make_grid(ims, nrow=train_config['num_grid_rows'])\n        img = torchvision.transforms.ToPILImage()(grid)\n        if not os.path.exists(os.path.join(train_config['task_name'], 'samples')):\n            os.mkdir(os.path.join(train_config['task_name'], 'samples'))\n        img.save(os.path.join(train_config['task_name'], 'samples', 'x0_{}.png'.format(i)))\n        img.close()\n\n\ndef infer():\n#     # Read the config file #\n#     with open(args.config_path, 'r') as file:\n#         try:\n#             config = yaml.safe_load(file)\n#         except yaml.YAMLError as exc:\n#             print(exc)\n#     print(config)\n#     ########################\n    \n    diffusion_config = config['diffusion_params']\n    model_config = config['model_params']\n    train_config = config['train_params']\n    \n    # Load model with checkpoint\n    model = Unet(model_config).to(device)\n    model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n                                                  train_config['ckpt_name']), map_location=device))\n    model.eval()\n    \n    # Create the noise scheduler\n    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n                                     beta_start=diffusion_config['beta_start'],\n                                     beta_end=diffusion_config['beta_end'])\n    with torch.no_grad():\n        sample(model, scheduler, train_config, model_config, diffusion_config)\n\n\nif __name__ == '__main__':\n#     parser = argparse.ArgumentParser(description='Arguments for ddpm image generation')\n#     parser.add_argument('--config', dest='config_path',\n#                         default='config/default.yaml', type=str)\n#     args = parser.parse_args()\n    infer()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T10:04:22.868248Z","iopub.execute_input":"2024-09-06T10:04:22.868642Z","iopub.status.idle":"2024-09-06T10:06:41.689284Z","shell.execute_reply.started":"2024-09-06T10:04:22.868596Z","shell.execute_reply":"2024-09-06T10:06:41.68835Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r output.zip ./\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T10:07:03.122051Z","iopub.execute_input":"2024-09-06T10:07:03.122432Z","iopub.status.idle":"2024-09-06T10:07:13.338479Z","shell.execute_reply.started":"2024-09-06T10:07:03.122395Z","shell.execute_reply":"2024-09-06T10:07:13.33752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}